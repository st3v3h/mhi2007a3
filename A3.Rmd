---
title: "Assignment 3"
author: "Steve Hawley"
date: "December 12, 2018"
output:
  word_document: default
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

## Section 0: Load the data and packages

```{r get_data, message=FALSE, warning=FALSE, results='markup'}
library(tidyverse) #for data management
library(foreign) #for importing spss data
library(car) #for leven's test
library(sjstats) #for eta squared
library(effects) #for adjusted means in ancova
library(BaylorEdPsych) #for logisitic regression effect size
#library(QuantPsyc) # For Beta values -- conflicts with dplyr
#library(ppcor) #for pr values -- conflicts with dplyr

# import the spss file. Label the factors
data<-read.spss("Data+for+assignments.sav",to.data.frame = TRUE, add.undeclared.levels = "no")
# take a look at the data. See what the fields are
head(data)
#inspect the data types
str(data)
```
## Section 1: ANCOVA

#### **Research Question:** Is there a difference in confidence between males and females when we control for overall life satisfaction?

```{r ancova, results='markup'}
#reference: http://faculty.missouri.edu/huangf/data/quantf/ancova_in_r_handout.pdf

#subset the data for analysis
anc<-data %>% 
  select(gender, life, confid) %>% 
  drop_na(gender, life, confid)

#plot to look at homogeneity of regression
ggplot(anc, aes(x=life, y=confid, color=gender)) +
  geom_point(alpha = 0.15) +
  geom_smooth(method = "lm")
#slopes look reasonably parellel 

#Check descriptives
anc %>% 
  group_by(gender) %>% 
  summarize(mean = mean(confid),
            sd = sd(confid),
            n = n())

t.test(life~gender, data = anc)
#t test is n.s. genders are not different on covariate 

leveneTest(anc$confid,anc$gender, center = mean)
#levene's is n.s. therefore assumption of homogeneity of variance is met

#test to look for equality of slopes
mod1 <- aov(confid~gender + life + gender:life, data = anc)
summary(mod1)
EtaSq(mod1)
#interaction of gender:life is n.s. so the slope across groups is not different

mod2 <- aov(confid~life + gender, data = anc)
summary(mod2)

#effect size. Don't need to report results for covariate 
EtaSq(mod2)

#adjusted means
effect("gender", mod2)


#CHECKING ASSUMPTIONS

hist(residuals(mod2))
#histogram of residuals in normal

plot(fitted(mod2), residuals(mod2))

qqnorm(residuals(mod2))


##########Alternative approach###########
mod1 <- lm(data = anc, confid~life + gender + life:gender)
Anova(mod1, type = "II")
#Interaction is not significant, so the slope across groups is not different
mod2 <- lm(data = anc, confid~life + gender)
Anova(mod2, type = "II")
```
#####Results (APA style)
To investigate whether gender has any effect on confidence, a one-way ANCOVA was conducted. The independant variable, gender, included two levels, male and female. The dependant variable was a confidence score from a self reported assessment and the covariate was a life satisfaction score. A preliminary analysis evaluating the homogeneity-of-slopes assumption indicated that the relationship between the covariate and the dependent variable did not differe significantly as a function of the independent variable, F(1,652) = 1.855, MSE = 127, p = 0.174, partial eta_squared = 0.003. The ANCOVA was significant, F(1,653) = 43.9, MSE = 2999, p <0.01, partial eta_squared = 0.06. The strength of relationship between gender and confidence was small, as assessed by partial eta_squared, with the gender accounting for 6% of the variance in the dependent variable, holding life score constant. The means of the confidence scores adjusted for initial differences in life satisfaction scores were ordered in the following way: males (M = 65.60) and females (M = 61.17). Follow up tests were not conducted as gender contained only two factors.

## Section 2: Logistic Regression

#### **Research Question:** To what extent does the age and working status predict the politcal affiliation of a participant?

```{r logreg, results='markup'}

#subset the data for analysis -- DV = polaff ("independent" factor filtered out); cat IV = work; contin IV = age
lr <- data %>% 
        select(work, age, polaff) %>% 
        filter(polaff=="Republican" | polaff=="Democrat") %>% 
        drop_na(work, age)
#dropping the empty factor
lr$polaff <- factor(lr$polaff)

#let's have a look to see if there's any issues
summary(lr)
#nothing out of the ordinary

#set up the model and get the output
model <- glm(polaff~work + age,
             family = binomial(link = 'logit'),
             data = lr)
summary(model)

modelx <- glm(polaff~work,
             family = binomial(link = 'logit'),
             data = lr)
summary(modelx)


#is there a significant reduction in error by adding the predictors of work and age (i.e., Null deviance vs. Residual deviance)?
chidiff <- model$null.deviance - model$deviance
dfdiff <- model$df.null - model$df.residual
chidiff
dfdiff
pchisq(chidiff,dfdiff, lower.tail = F)
#difference is n.s. (though trending towards significance) therefore predictors do not improve model

#Effect size
PseudoR2(model)

#checking which group is the lower group
table(lr$polaff)

#look at the % correct from the model
#fitted values give probability of being Republican
correct <- model$fitted.values
#Thresholding values into bins where 0.5 is equal split between groups
binarycorrect <- ifelse(correct >0.5,1,0)
binarycorrect <- factor(binarycorrect,
                        levels = c(0,1),
                        labels = c("Republican pred", "Democrat pred"))

table(lr$polaff, binarycorrect)
#from the table, 6 republicans predicted correctly, 14 incorrectly
#207 Democrats predicted correctly, 128 predicted incorrectly

#To get probabiliy of correctly guessing using model:
#Republican
6/(6+128)*100
#Democrat
207/(14+207)*100
#Overall
(6+207)/nrow(lr)*100

#to get Odds Ratios
exp(model$coefficients)

```
#####Results (APA style)
A logistic regression was conducted to explore whether age and working status are related to political affiliation. The model had a non-significant fit X^2^(3) = 7.63, p = 0.054, Nagelkerke R^2^ = 0.029. The model correctly classifies 60% of participants in the sample. The classification of Democrats (93.7%) was much better than Republicans (4.5%). Of two predictor variables considered in this analysis, only age was significantly related to political affiliation. Specifically, with each additional year in age, there was approximately a 2% increased chance of a Democrat political affiliation.


## Section 3: Hierarchical Multiple Regression

#### **Research Question:** After controlling for demographic variables (age and education), does life satisfaction of the participant predict their confidence?

```{r hmr, results='markup'}
#For this section I followed this tutorial:
#https://www.youtube.com/watch?v=zFEP-lJ1LD0&feature=youtu.be

#subset the data for this analysis
hmr<- data %>% 
  select(age,educ,life,confid) %>% 
  drop_na(age,educ,life,confid)

#Let's take a look for anythign weird
summary(hmr)

#running the final model to get assumption checks
output <- lm(confid~life + educ + age, data=hmr)
summary(output)

####DEALING WITH OUTLIERS####
#mahalanobis
mahal <- mahalanobis(hmr,
                     colMeans(hmr),
                     cov(hmr))

cutoff <- qchisq(1-.001, ncol(hmr))
cutoff  
ncol(hmr)#df

#how many outliers are there?
badmahal <- as.numeric(mahal>cutoff)
table(badmahal)  

#leverage
k = 3
leverage <-  hatvalues(output)
cutleverage <-  (2*k+2)/nrow(hmr)
cutleverage
badleverage <-  as.numeric(leverage>cutleverage)
table(badleverage)
#54 people exceed the leverage cutoff

#cooks
cooks <-  cooks.distance(output)
cutcooks <-  4/(nrow(hmr)-k-1)
cutcooks
badcooks <-  as.numeric(cooks>cutcooks)
table(badcooks)
#32 people exceed the cook's cutoff score

##Total outliers -- general rule of thumb: people with 2 or more outlier indicators should be excluded
totalout <-  badmahal + badleverage + badcooks
table(totalout)
#15 people have 2 indicators. We should exclude them.
hmr_xo <- subset(hmr, totalout<2)

#rerun analysis without outliers
output <- lm(confid~life + educ + age, data=hmr_xo)
summary(output)

#additivity. Check to see if IVs are correlated. 
correl <-  cor(hmr_xo, use = "pairwise.complete.obs")
correl
summary(output, correlation = T)

#assumption set up
standardized  <-  rstudent(output)
fitted <-  scale(output$fitted.values)

#normality
hist(standardized)

#linearity
qqnorm(standardized)
abline(0,1)

#homogeneity and homoscedasticity
plot(fitted,standardized)
abline(0,0)
abline(v=0)

##First model of just demographics
model1 <- lm(confid~age + educ, data = hmr_xo)
summary(model1)

#to get Beta values
QuantPsyc::lm.beta(model1)

#to get pr values
partials <- ppcor::pcor(hmr_xo[,c(1,2,4)], method = "pearson")
partials$estimate^2

#adding in the next IV to the model 
model2 <- lm(confid~age + educ + life, data = hmr_xo)
summary(model2)

#compare models to see if there's a difference
anova(model1, model2)

#get beta for new model
QuantPsyc::lm.beta(model2)

#get pr values for new model
partials <- ppcor::pcor(hmr_xo, method = "pearson")
partials$estimate^2

#plot to show how accurate our model is
cleanup <- theme(panel.grid.major = element_blank(),
                 panel.grid.minor = element_blank(),
                 panel.background = element_blank(),
                 axis.line = element_line(colour = "black"),
                 legend.key = element_rect(fill = "white"),
                 text = element_text(size = 15))

fitted <- model2$fitted.values

scatter <- ggplot(hmr_xo, aes(fitted, confid))
scatter + 
  cleanup +
  geom_point() +
  geom_smooth(method = "lm", color = "black") +
  xlab("Age + Education + Life Satisfaction") +
  ylab("Confidence Score")


```
#####Results (APA style)
  Age, education and life satisfaction score were used to predict a participant's confidence score. The data were screened for assumptions, and 15 participants were removed as outliers due to high Mahalanobis, Cook's and/or leverage scores. Linearity, normality, multicollinearity, homogeneity and homoscedasticity were all met.
  Age and education were entered first into a hierarchical regression to control for demographic differences in confidence. Overall, this model was significant, indicating that demographics predict a participant's confidence score, F(2,608) = 8.48, p<0.01, R^2^ = 0.03. Age was a stronger predictor of confidence, B = 0.12, t(608) = 3.186, p<0.01, pr^2^ = 0.02, which showed that participants are likely to be more confident as they get older. Education was also positively related to confidence, B = 0.09, t(608) = 2.27, p = 0.02, pr^2^ = 0.008; therefore, more educated participants tend to be more confident. Next, life satisfaction score was added in a second step to examine its predictive value after controlling for demographic variables. The addition of this variable was significant DELTA_F(1,607) = 762.95, p < 0.001, DELTA_R^2^ = 0.539 (model 2 - model 1). Life satisfaction scores was the highest predictor of confidence score where participants with a higher life satisfaction score also had higher confidence scores B = 0.75, t(607) = 27.62, p<0.001, pr^2^ = 0.557