---
title: "Assignment 3"
author: "Steve Hawley"
date: "December 12, 2018"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

## Section 0: Load the data and packages

```{r get_data, message=FALSE, warning=FALSE, results='markup'}
library(tidyverse)
library(foreign) #for importing spss data
library(summarytools) #for descriptive and cross tables
library(car) #for leven's test
library(sjstats) #for eta squared
library(effects) #for adjusted means in ancova
library(BaylorEdPsych) #for logisitic regression effect size

# import the spss file. Label the factors
data<-read.spss("Data+for+assignments.sav",to.data.frame = TRUE, add.undeclared.levels = "no")
# take a look at the data. See what the fields are
head(data)
#inspect the data types
str(data)
```
## Section 1: ANCOVA

#### **Research Question:** Is there a difference in confidence between males and females when we control for overall life satisfaction?

```{r ancova}
#reference: http://faculty.missouri.edu/huangf/data/quantf/ancova_in_r_handout.pdf

#subset the data for analysis
anc<-data %>% 
  select(gender, life, confid) %>% 
  drop_na(gender, life, confid)

#plot to look at homogeneity of regression
ggplot(anc, aes(x=life, y=confid, color=gender)) +
  geom_point(alpha = 0.15) +
  geom_smooth(method = "lm")
#slopes look reasonably parellel 

#Check descriptives
anc %>% 
  group_by(gender) %>% 
  summarize(mean = mean(confid),
            sd = sd(confid),
            n = n())

t.test(life~gender, data = anc)
#t test is n.s. genders are not different on covariate 

leveneTest(anc$confid,anc$gender, center = mean)
#levene's is n.s. therefore assumption of homogeneity of variance is met

#test to look for equality of slopes
mod1 <- aov(confid~gender + life + gender:life, data = anc)
summary(mod1)
EtaSq(mod1)
#interaction of gender:life is n.s. so the slope across groups is not different



mod2 <- aov(confid~life + gender, data = anc)
summary(mod2)

#effect size. Don't need to report results for covariate 
EtaSq(mod2)

#adjusted means
effect("gender", mod2)


#CHECKING ASSUMPTIONS

hist(residuals(mod2))
#histogram of residuals in normal

plot(fitted(mod2), residuals(mod2))

qqnorm(residuals(mod2))


##########Alternative approach###########
mod1 <- lm(data = anc, confid~life + gender + life:gender)
Anova(mod1, type = "II")
#Interaction is not significant, so the slope across groups is not different
mod2 <- lm(data = anc, confid~life + gender)
Anova(mod2, type = "II")
```

To investigate whether gender has any effect on confidence a one-way ANCOVA was conducted. The independant variable, gender, included two levels, male and female. The dependant variable was a confidence score from a self reported assessment and the covariate was a life satisfaction score. A preliminary analysis evaluating the homogeneity-of-slopes assumption indicated that the relationship between the covariate and the dependent variable did not differe significantly as a function of the independent variable, F()

## Section 2: Logistic Regression

#### **Research Question:** To what extent does the age and working status predict the politcal affiliation of a participant?

```{r logreg}

#subset the data for analysis -- DV = polaff; cat IV = work; contin IV = age
lr <- data %>% 
        select(work, age, polaff) %>% 
        filter(polaff=="Republican" | polaff=="Democrat") %>% 
        drop_na(work, age)
#dropping the empty factor
lr$polaff <- factor(lr$polaff)

#let's have a look to see if there's any issues
summary(lr)

# #balance polaff
# t <- lr %>% group_by(polaff) %>% 
#   sample_n(100)
# table(t$polaff)
# summary(t)

#set up the model and get the output
model <- glm(polaff~work + age,
             family = binomial(link = 'logit'),
             data = lr)
summary(model)

#is there a significant reduction in error by adding the predictors of work and age (i.e., Null deviance vs. Residual deviance)?
chidiff <- model$null.deviance - model$deviance
dfdiff <- model$df.null - model$df.residual
chidiff
dfdiff
pchisq(chidiff,dfdiff, lower.tail = F)
#difference is n.s. (though trending towards significance) therefore predictors do not improve model

#Effect size
PseudoR2(model)


```

## Section 3: Hierarchical Multiple Regression

#### **Research Question:** After controlling for demographic variables (age and education), does life satisfaction of the participant predict their confidence?

```{r hmr, results='asis'}
#For this section I followed this fantastic tutorial:
#https://www.youtube.com/watch?v=zFEP-lJ1LD0&feature=youtu.be

#subset the data for this analysis
hmr<- data %>% 
  select(age,educ,life,confid) %>% 
  drop_na(age,educ,life,confid)

#Let's take a look for anythign weird
summary(hmr)

#running the final model to get assumption checks
output <- lm(confid~life + educ + age, data=hmr)
summary(output)

####dealing with outliers
#mahalanobis
mahal <- mahalanobis(hmr,
                     colMeans(hmr),
                     cov(hmr))

cutoff <- qchisq(1-.001, ncol(hmr))
cutoff  
ncol(hmr)#df

#how many outliers are there?
badmahal <- as.numeric(mahal>cutoff)
table(badmahal)  

#leverage
k = 3
leverage <-  hatvalues(output)
cutleverage <-  (2*k+2)/nrow(hmr)
cutleverage
badleverage <-  as.numeric(leverage>cutleverage)
table(badleverage)
#54 people exceed the leverage cutoff

#cooks
cooks <-  cooks.distance(output)
cutcooks <-  4/(nrow(hmr)-k-1)
cutcooks
badcooks <-  as.numeric(cooks>cutcooks)
table(badcooks)
#32 people exceed the cook's cutoff score

##Total outliers -- general rule of thumb: people with 2 or more outlier indicators should be excluded
totalout <-  badmahal + badleverage + badcooks
table(totalout)
#15 people have 2 indicators. We should exclude them.
hmr_xo <- subset(hmr, totalout<2)

#rerun analysis without outliers
output <- lm(confid~life + educ + age, data=hmr_xo)
summary(output)

#additivity. Check to see if IVs are correlated. 
correl <-  cor(hmr_xo, use = "pairwise.complete.obs")
correl
summary(output, correlation = T)

#assumption set up
standardized  <-  rstudent(output)
fitted <-  scale(output$fitted.values)

#normality
hist(standardized)

#linearity
qqnorm(standardized)
abline(0,1)

#homogeneity and homoscedasticity
plot(fitted,standardized)
abline(0,0)
abline(v=0)

##First model of just demographics
model1 <- lm(confid~age + educ, data = hmr_xo)
summary(model1)

#to get Beta values
library(QuantPsyc)
lm.beta(model1)

#to get pr values
library(ppcor)
partials <- pcor(hmr_xo[,c(1,2,4)], method = "pearson")
partials$estimate^2

#adding in the next IV to the model 
model2 <- lm(confid~age + educ + life, data = hmr_xo)
summary(model2)

#compare models to see if there's a difference
anova(model1, model2)

#get beta for new model
lm.beta(model2)

#get pr values for new model
partials <- pcor(hmr_xo, method = "pearson")
partials$estimate^2

#plot to show how accurate our model is
cleanup <- theme(panel.grid.major = element_blank(),
                 panel.grid.minor = element_blank(),
                 panel.background = element_blank(),
                 axis.line = element_line(colour = "black"),
                 legend.key = element_rect(fill = "white"),
                 text = element_text(size = 15))

fitted <- model2$fitted.values

scatter <- ggplot(hmr_xo, aes(fitted, confid))
scatter + 
  cleanup +
  geom_point() +
  geom_smooth(method = "lm", color = "black") +
  xlab("Age + Education + Life Satisfaction") +
  ylab("Confidence Score")


```